# Web Exploitation

- [Web Exploitation](#web-exploitation)
  - [Generic Tools](#generic-tools)
    - [Scraping tool (webctf)](#scraping-tool-webctf)
  - [Installation](#installation)
    - [Web Postman (User Agent/ Browser)](#web-postman-user-agent-browser)
  - [Downloading entire website](#downloading-entire-website)
  - [Request methods](#request-methods)
    - [HEAD requests](#head-requests)
      - [Examples (Head requests)](#examples-head-requests)
  - [Cookies](#cookies)
  - [Inspect](#inspect)
    - [Inspector](#inspector)
    - [Style Editor](#style-editor)
    - [Network](#network)
    - [Index](#index)
      - [Robots.txt](#robotstxt)
        - [Examples (Robots.txt)](#examples-robotstxt)
      - [Apache server](#apache-server)
        - [Examples (Indexing Apache Server)](#examples-indexing-apache-server)
      - [MacOS file](#macos-file)
    - [User agent/ browser](#user-agent-browser)
  - [Login Page Redirect](#login-page-redirect)
  - [Directory Traversal](#directory-traversal)
    - [Examples (Directory Traversal)](#examples-directory-traversal)

## Generic Tools

### Scraping tool (webctf)

[Web-CTF-Help](https://github.com/xnomas/web-ctf-help)

Usage

```bash
usage: webctf [-h] [-v] [--comments] [--scripts] [--images] [--headers] [--cookies COOKIES] [--flags FLAGS] [-f] url

positional arguments:
  url            URL of the target website

optional arguments:
  -h, --help     show this help message and exit
  -v, --version  show program's version number and exit
  --comments     only display HTML comments (default: False)
  --scripts      only display script sources (default: False)
  --images       only display image sources (default: False)
  --headers      only display interesting response headers (combine with -f to display all) (default: False)
  --cookies COOKIES  add cookies to your request in the following format "name=value;name2=value2" (default: None)
  --flags FLAGS      search for a flag hidden on the website in the following format "pattern", and will be searched as "pattern\{*\}" (default: None)
  -f, --full     enable full output for all options (default: False)
```

Only display HTML comments and script sources:

```bash
webctf --comments --scripts https://example.com
```

Only display interesting headers:

```bash
webctf --headers https://example.com
```

Display all headers:

```bash
webctf --headers -f https://example.com
```

Send a request with a cookie and only get comments

```bash
webctf --cookies "name1=value1;name2=value2" --comments https://example.com 
```

## Installation

```bash
python3 -m pip install git+https://github.com/xnomas/web-ctf-help.git
```


### Web Postman (User Agent/ Browser)

[Web Postman](https://web.postman.co/workspace/My-Workspace~ae97d7b2-6e21-4715-802e-e28ca781b2f6/request/34515252-b5d65257-475e-4408-b70d-3c196dd40f1b)

## Downloading entire website

Download the whole website with wget (or other) including all its downloadable content
Search through the entire sources using wget
Using -m (mirror) instead of -r is preferred as it intuitively downloads assets and you don't have to specify recursion depth, using mirror generally determines the correct depth to return a functioning site.

The commands -p -E -k ensure that you're not downloading entire pages that might be linked to (e.g. Link to a Twitter profile results in you downloading Twitter code) while including all pre-requisite files (JavaScript, css, etc.) that the site needs. Proper site structure is preserved as well (instead of one big .html file with embedded scripts/stylesheet that can sometimes be the output.

It's fast, I have never had to limit anything to get it to work and the resulting directory looks better than simply using the -r "url" arg and provides better insight into how the site was put together, especially if you're reverse-engineering for educational purposes.

If you end up getting kicked from the site's IP, or the download stops, try running the same command, but with: --wait="duration" enabled. This adds a duration between requests so as not to trigger any DDoS flags on their end.

```bash
wget -mpEk "url"
grep -r pico .
```

## Request methods

### HEAD requests

#### Examples (Head requests)

1. You can see there are two different methods used. "GET" and "POST" so the hint is probably referring to a third method and we can see "HEAD" popping out in the title. Let's try a "HEAD" request.

    ```bash
    curl -I HEAD -i http://mercury.picoctf.net:53554/index.php
    ```

## Cookies

1. [Open Chrome DevTools](https://developer.chrome.com/docs/devtools/open).
2. Open **Application** > **Storage** > **Cookies** and select an origin.
3. Edit the cookie and refresh the page
   - Loop through different values of cookies to find the desired return output

    ```python
    import requests
    url = "http://mercury.picoctf.net:54219/check"

    for i in range(0, 20):
        text = str(i)
        cookies = {
            'name': text
        }

        r = requests.get(url, cookies=cookies)
        result = r.text.split(
            "<p style=\"text-align:center; font-size:30px;\"><b>")[1].split("</b>")[0]
        print("[+] Testing Cookie:{} | Result: {}".format(i, result))
        if 'I love' not in result:
            print(r.text.split("<code>")[1].split("</code>")[0])
            break
    ```

## Inspect

### Inspector

Read the HTML document

### Style Editor

Check css files

### Network

Double click row to view js files etc

### Index

#### Robots.txt

Tells you the disallowed page, you can try to go into that page and find.

Add /robots.txt → Crawlers will always look for your robots.txt file in the root of your website.

We have to think about how Google can keep track of all these websites? 
It uses the Google web crawling/spider engine. But since devs don't want these spiders to reach and `index` every part of the website, we use a special file called `robots.txt`!

This file tells the crawlers what parts of the site are disallowed, and what User Agents are allowed to visit (among other things). Crawlers that listen to these files are called `polite`, when a crawler is not `polite` it could fall into something called a `spider trap`. More on that [here](https://www.techopedia.com/definition/5197/spider-trap).

##### Examples (Robots.txt)

1. Navigate to your domain, and just add " /robots.txt ". \
   https://www.contentkingapp.com/robots.txt

#### Apache server

Add /.htaccess

##### Examples (Indexing Apache Server)

1. I think this is an apache server... can you Access the next flag? \
   We now need to access the `.htaccess` file!\
   http://mercury.picoctf.net:5080/.htaccess

#### MacOS file

.DS_Store → add /.DS_Store

`.DS_Store`is a special MacOS file that stores information about the current folder. Like icon positioning etc. You may also see it if you unzip a file from a Mac user on a non-Mac computer. Kind of a token identifier of Mac computers.

### User agent/ browser

```python
import requests
url = 'http://mercury.picoctf.net:52362'
params = { 'User-Agent' : 'PicoBrowser' }
r = requests.get(url, params=params)
print(r.text)
```

Inspect → Network → reload → request headers for this site

[Web Postman](https://web.postman.co/workspace/My-Workspace~ae97d7b2-6e21-4715-802e-e28ca781b2f6/request/34515252-b5d65257-475e-4408-b70d-3c196dd40f1b)

Create a new collection → pre-request script → pm.request.headers.add({key: 'User-Agent', value: 'PicoBrowser'},{key:'Referer', value: 'http://mercury.picoctf.net:36622/'});

## Login Page Redirect

To see what the redirected page is about, we can use tools such as BurpSuite to see the contents of the page through the response before forwarding the page to the home page.

## Directory Traversal

### Examples (Directory Traversal)

1. We know that the website files live in /usr/share/nginx/html/ and the flag is at /flag.txt but the website is filtering absolute file paths.\
This website has the useful feature of reading any file we want it too, given its path. With file paths, a preceeding ./ means the current directory, and ../ means the enclosing directory. Since we know that we are in /usr/share/nginx/html/, and want to access /flag.txt, we can just use the path ../../../../flag.txt to read the flag.
